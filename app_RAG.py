# =========================================================
# Imports
# ---------------------------------------------------------
# - os: utilitaires syst√®me (existence de dossiers/fichiers)
# - re: nettoyage de texte via expressions r√©guli√®res
# - gradio: interface web pour l'app RAG
# - langchain / ollama / chroma: stack RAG (LLM, embeddings, vecteur store, prompt, chain)
# - text splitter: d√©coupage des documents en chunks
# =========================================================
import os
import re
import gradio as gr
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate

# =========================================================
# CONFIGURATION GLOBALE
# ---------------------------------------------------------
# - DATA_DIR: r√©pertoire contenant les fichiers d√©j√† pars√©s (Markdown/TXT)
# - CHROMA_DIR: r√©pertoire de persistance de l'index Chroma
# - embed_model: mod√®le d'embedding (Ollama)
# - llm: mod√®le de g√©n√©ration (Ollama)
# =========================================================
DATA_DIR = "/var/www/RAG/Data_parse"
CHROMA_DIR = "./chroma_index"

embed_model = OllamaEmbeddings(model="nomic-embed-text", base_url="http://localhost:11434")
llm = OllamaLLM(
    model="gpt-oss:latest",
    base_url="http://localhost:11434",
    temperature=0.1,
    num_ctx=8192,
    request_timeout=3000
)

# =========================================================
# UTILITAIRES TEXTE
# ---------------------------------------------------------
# clean_text:
# - compresse les espaces multiples
# - remplace l'espace ins√©cable par un espace normal
# - retire espaces en d√©but/fin
# =========================================================
def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    text = text.replace('\u00A0', ' ')
    return text.strip()

# =========================================================
# CHARGEMENT & NETTOYAGE DES DOCUMENTS
# ---------------------------------------------------------
# load_and_clean_docs:
# - charge tous les fichiers texte sous DATA_DIR (r√©cursif)
# - auto-d√©tecte l'encodage
# - nettoie le contenu via clean_text
# - retourne la liste de documents LangChain
# =========================================================
def load_and_clean_docs():
    loader = DirectoryLoader(
        DATA_DIR,
        glob="**/*",
        loader_cls=TextLoader,
        show_progress=True,
        use_multithreading=True,
        loader_kwargs={"autodetect_encoding": True}
    )
    docs = loader.load()
    for doc in docs:
        doc.page_content = clean_text(doc.page_content)
    return docs

# =========================================================
# INDEX VECTORIEL CHROMA
# ---------------------------------------------------------
# - Si CHROMA_DIR existe : on recharge l'index existant
# - Sinon : on cr√©e l'index √† partir des documents (split puis persist)
#   * chunk_size=500, overlap=50: compromis pr√©cision/rappel
# =========================================================
if os.path.exists(CHROMA_DIR):
    db = Chroma(persist_directory=CHROMA_DIR, embedding_function=embed_model)
else:
    print("üìö Cr√©ation de l‚Äôindex vectoriel...")
    docs = load_and_clean_docs()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)
    db = Chroma.from_documents(chunks, embed_model, persist_directory=CHROMA_DIR)
    db.persist()

# =========================================================
# RETRIEVER
# ---------------------------------------------------------
# - MMR (Maximal Marginal Relevance): √©quilibre pertinence/diversit√©
# - k=5: nombre final de passages retourn√©s
# - fetch_k=20: candidats initiaux avant MMR
# - lambda=0.5: √©quilibre MMR (0 = diversit√©, 1 = similarit√©)
# =========================================================
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20 , "lambda": 0.5}
)

# =========================================================
# PROMPT RAG
# ---------------------------------------------------------
# - Contexte inject√© depuis les documents r√©cup√©r√©s
# - Ton: direct, concis, factuel
# - Exigence: citer la source (ex: nom de fichier) quand info mentionn√©e
# - Si info absente: le dire clairement
# =========================================================
question_prompt = ChatPromptTemplate.from_template("""
Vous √™tes un assistant technique de la marque CWD. R√©pondez de mani√®re **directe, concise et strictement factuelle** √† la question pos√©e, en vous appuyant uniquement sur les documents fournis.

üìå Contraintes :
- √âvitez toute reformulation de la question
- **Ne donnez aucune recommandation g√©n√©rale ou commerciale**
- Ne r√©p√©tez pas d‚Äôinformation inutile ou hors sujet
- Si une information est mentionn√©e, citez **clairement sa source** (ex. : nom du fichier)
- Si l‚Äôinformation est absente, dites-le clairement, sans supposition

=== CONTEXTE DOCUMENTAIRE ===
{context}

=== QUESTION ===
{question}

=== R√âPONSE COURTE ===
""")

# =========================================================
# FORMATTEUR DE CONTEXTE
# ---------------------------------------------------------
# format_docs:
# - concat√®ne les contenus des documents s√©lectionn√©s
# - s√©par√©s par deux sauts de ligne (lisibilit√©)
# =========================================================
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# =========================================================
# CHA√éNE RAG (retrieval -> prompt -> LLM -> parseur)
# ---------------------------------------------------------
# - "context": passe par le retriever puis formatage
# - "question": transite telle quelle (RunnablePassthrough)
# - StrOutputParser: standardise la sortie sous forme de texte
# =========================================================
qa_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | question_prompt
    | llm
    | StrOutputParser()
)

# =========================================================
# FONCTIONS D'INTERFACE M√âTIER
# ---------------------------------------------------------
# rag_interface(query):
# - g√®re l'absence de saisie
# - invoque la cha√Æne RAG
# - renvoie la r√©ponse et un bloc "sources" indicatif
#
# resolve_question(selected, typed, action):
# - choisit la question effective selon l'origine (s√©lecteur vs saisie)
# =========================================================
def rag_interface(query):
    if not query:
        return "‚ö†Ô∏è Veuillez entrer une question.", ""
    try:
        answer = qa_chain.invoke(query)
        return f"üí° **R√©ponse :**\n\n{answer}", f"üìÇ **Fichiers consult√©s :**\n(index Chroma local)"
    except Exception as e:
        return f"‚ùå Erreur : {str(e)}", ""

def resolve_question(selected, typed, action):
    if action == "typed":
        return typed
    elif action == "selected":
        return selected
    else:
        return typed or selected

# =========================================================
# UI GRADIO
# ---------------------------------------------------------
# - En-t√™te: logo + titre + sous-titre
# - Colonne gauche: dropdown de questions + zone de texte + bouton
# - Colonne droite: √©tats ("thinking") + r√©ponse + sources
# - Logique:
#   * change() met √† jour last_action (selected/typed)
#   * click() affiche "thinking", ex√©cute RAG, masque "thinking"
# - root_path="/rag_cwd": utile derri√®re un reverse proxy
# =========================================================
with gr.Blocks(title="üß† Assistant CWD", theme=gr.themes.Soft(primary_hue="red", secondary_hue="gray")) as demo:
    gr.Markdown("""
    <div style="text-align:center">
        <img src="cwd_logo.png" width="150">
        <h2 style="color:#8B0000;">üß† Assistant Technique & Commercial CWD</h2>
        <p>Posez votre question ci-dessous pour obtenir une r√©ponse experte bas√©e sur vos documents internes.</p>
    </div>
    """)

    with gr.Row():
        with gr.Column():
            # S√©lecteur de questions pr√©d√©finies (facilite les tests)
            question_selector = gr.Dropdown(
                label="üìã Choisissez une question (optionnel)",
                choices=[
                ],
                interactive=True
            )
            # Zone de saisie libre
            question_input = gr.Textbox(
                label="üí¨ Ou tapez votre propre question",
                placeholder="Ex : Quelle est la dur√©e de vie moyenne d‚Äôune selle ?",
                lines=3
            )
            # Bouton de lancement
            run_button = gr.Button("üîç G√©n√©rer une r√©ponse")
            # √âtat interne pour savoir si l'utilisateur a tap√© ou s√©lectionn√©
            last_action = gr.State()

        with gr.Column():
            # Zone "thinking" (affich√©e pendant l'inf√©rence)
            thinking_output = gr.Markdown(visible=False)
            # R√©ponse finale
            answer_output = gr.Markdown()
            # Sources/infos compl√©mentaires
            sources_output = gr.Markdown()

    # --- D√©tection de l'action utilisateur (met √† jour last_action) ---
    question_selector.change(
        fn=lambda val: "selected",
        inputs=question_selector,
        outputs=last_action
    )

    question_input.change(
        fn=lambda val: "typed",
        inputs=question_input,
        outputs=last_action
    )

    # --- Affichage d'un √©tat "r√©flexion" avant l'appel RAG ---
    def trigger_thinking(selector, typed, action):
        return gr.update(visible=True, value="‚è≥ L‚Äôassistant r√©fl√©chit..."), selector, typed, action

    # --- Pipeline clic:
    # 1) affiche "thinking"
    # 2) ex√©cute la RAG sur la question r√©solue
    # 3) masque "thinking"
    run_button.click(
        fn=trigger_thinking,
        inputs=[question_selector, question_input, last_action],
        outputs=[thinking_output, question_selector, question_input, last_action]
    ).then(
        fn=lambda selector, typed, action: rag_interface(resolve_question(selector, typed, action)),
        inputs=[question_selector, question_input, last_action],
        outputs=[answer_output, sources_output]
    ).then(
        fn=lambda: gr.update(visible=False),
        outputs=[thinking_output]
    )

# =========================================================
# LANCEMENT DE L'APP
# ---------------------------------------------------------
# - server_name="127.0.0.1": √©coute locale
# - server_port=6060: port HTTP
# - debug=True: logs d√©taill√©s (utile dev)
# - root_path="/rag_cwd": chemin racine si derri√®re un proxy
# =========================================================
if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=6050, debug=True, root_path="/rag_cwd")
