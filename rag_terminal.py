import os
import re
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate

# =========================================================
# CONFIGURATION GLOBALE
# ---------------------------------------------------------
# DATA_DIR   : r√©pertoire contenant les documents pr√©trait√©s
# CHROMA_DIR : r√©pertoire o√π sera stock√© l‚Äôindex Chroma persistant
# =========================================================
DATA_DIR = "/var/www/RAG/Data_parse"
CHROMA_DIR = "/var/www/RAG/chroma_index"

# =========================================================
# EMBEDDING MODEL (Ollama)
# ---------------------------------------------------------
# - Utilise "nomic-embed-text" pour transformer les documents en vecteurs
# - base_url = Ollama local
# =========================================================
embed_model = OllamaEmbeddings(
    model="nomic-embed-text", 
    base_url="http://localhost:11434"
)

# =========================================================
# LLM (Ollama - Llama3)
# ---------------------------------------------------------
# - Mod√®le utilis√© : "llama3:latest"
# - Contexte max = 8192 tokens
# - Temp√©rature basse (0.1) pour r√©ponses pr√©cises et d√©terministes
# =========================================================
llm = OllamaLLM(
    model="llama3:latest",
    base_url="http://localhost:11434",
    temperature=0.1,
    num_ctx=8192,
    request_timeout=3000
)

# =========================================================
# FONCTION DE NETTOYAGE TEXTE
# ---------------------------------------------------------
# - Supprime espaces multiples
# - Remplace espaces ins√©cables par des espaces normaux
# - Retourne texte nettoy√©
# =========================================================
def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    text = text.replace('\u00A0', ' ')
    return text

# =========================================================
# CHARGEMENT + NETTOYAGE DES DOCUMENTS
# ---------------------------------------------------------
# - DirectoryLoader : charge tous les fichiers texte sous DATA_DIR
# - TextLoader      : d√©tecte automatiquement l'encodage
# - Nettoyage du contenu avant indexation
# =========================================================
def load_and_clean_docs():
    loader = DirectoryLoader(
        DATA_DIR,
        glob="**/*",
        loader_cls=TextLoader,
        show_progress=True,
        use_multithreading=True,
        loader_kwargs={"autodetect_encoding": True}
    )
    docs = loader.load()
    for doc in docs:
        doc.page_content = clean_text(doc.page_content)
    return docs

# =========================================================
# CONSTRUCTION OU RECHARGEMENT DE L'INDEX CHROMA
# ---------------------------------------------------------
# - Si index existe d√©j√† : on le recharge
# - Sinon :
#   * On charge/nettoie les docs
#   * On les d√©coupe en chunks (500 tokens, overlap 50)
#   * On construit un nouvel index Chroma et on le persiste
# =========================================================
if os.path.exists(CHROMA_DIR):
    db = Chroma(persist_directory=CHROMA_DIR, embedding_function=embed_model)
else:
    print("üìö Cr√©ation de l‚Äôindex vectoriel...")
    docs = load_and_clean_docs()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)
    db = Chroma.from_documents(chunks, embed_model, persist_directory=CHROMA_DIR)
    db.persist()

# =========================================================
# RETRIEVER
# ---------------------------------------------------------
# - M√©thode : MMR (Maximal Marginal Relevance)
#   * k=5     : nombre final de passages retourn√©s
#   * fetch_k : 20 candidats initiaux
#   * lambda=0.5 : √©quilibre entre pertinence & diversit√©
# =========================================================
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5 , "fetch_k": 20 , "lambda": 0.5}
)

# =========================================================
# PROMPT RAG
# ---------------------------------------------------------
# - Ton : expert, pr√©cis, direct
# - Contraintes :
#   * Pas d'invention
#   * Pas de reformulation de la question
#   * Pas de conseils hors sujet
#   * Obligation de citer la source (nom du document)
# =========================================================
question_prompt = ChatPromptTemplate.from_template("""
Vous √™tes un assistant expert de la marque CWD. R√©pondez  en vous basant sur les documents fournis.

Contraintes :
- R√©pondez de mani√®re **directe, pr√©cise**
- **N'inventez jamais** d'informations non pr√©sentes dans les documents
- **Ne reformulez pas la question**
- **N‚Äôajoutez pas de conseils, g√©n√©riques ou hors sujet**
- Si une information est utilis√©e, citez sa **source (nom du document)**


=== CONTEXTE ===
{context}

=== QUESTION ===
{question}

=== R√âPONSE COURTE ===
""")

# =========================================================
# FORMATAGE DU CONTEXTE
# ---------------------------------------------------------
# - Concat√®ne le contenu des documents r√©cup√©r√©s
# - S√©par√©s par deux sauts de ligne pour lisibilit√©
# =========================================================
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# =========================================================
# CHA√éNE QA (RAG)
# ---------------------------------------------------------
# - √âtapes :
#   1) "context" : retriever + format_docs
#   2) "question" : passe telle quelle (RunnablePassthrough)
#   3) Injection dans le prompt
#   4) LLM g√©n√®re r√©ponse
#   5) StrOutputParser : formate la sortie texte
# =========================================================
qa_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | question_prompt
    | llm
    | StrOutputParser()
)

# =========================================================
# BOUCLE TERMINAL (Interface CLI)
# ---------------------------------------------------------
# - L'utilisateur saisit une question
# - R√©ponse g√©n√©r√©e via qa_chain
# - Quitter avec "exit" / "quit" / "q"
# =========================================================
print("\n‚úÖ Syst√®me pr√™t. Posez vos questions (ou tapez 'exit' pour quitter).\n")
while True:
    try:
        query = input("‚ùì Votre question : ")
        if query.strip().lower() in ["exit", "quit", "q"]:
            print("üëã Fin du programme.")
            break
        response = qa_chain.invoke(query)
        print("\nüß† R√©ponse :\n", response)
        print("-" * 60)
    except KeyboardInterrupt:
        print("\nüëã Interrompu.")
        break
    except Exception as e:
        print("‚ùå Erreur :", e)
