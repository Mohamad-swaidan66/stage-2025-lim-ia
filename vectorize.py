# builder.py (LlamaIndex -> Chroma, partag√© avec LangChain)
"""
Ingestion des documents puis construction d'un index VectorStoreIndex
persist√© dans Chroma, afin d'√™tre r√©utilis√© par LlamaIndex ET LangChain.
"""

# =========================================================
# Imports
# ---------------------------------------------------------
# os        : v√©rification d'existence de r√©pertoires
# chromadb  : client Chroma (vecteur store persistant)
# llama_index.core : objets de base (Index, Reader, Settings, Storage)
# Ollama    : LLM & Embeddings via Ollama (serveur local)
# SentenceSplitter : d√©coupage en chunks
# ChromaVectorStore: adaptation LlamaIndex <-> Chroma
# =========================================================
import os
import chromadb
from llama_index.core import (
    VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext
)
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.chroma import ChromaVectorStore

# =========================================================
# Configuration chemins & collection
# ---------------------------------------------------------
# DATA_DIR   : r√©pertoire source des documents √† indexer
# CHROMA_DIR : r√©pertoire du stockage persistant Chroma (HNSW)
# COLLECTION : nom logique de la collection Chroma
# =========================================================
DATA_DIR = "/var/www/RAG/Data_parse/"
CHROMA_DIR = "/var/www/RAG/chroma_index"
COLLECTION = "cwd_knowledge"

# =========================================================
# 1) Mod√®les (param√©trage global via Settings)
# ---------------------------------------------------------
# - Embeddings : "nomic-embed-text" (Ollama)
# - LLM        : "llama3:latest" (Ollama)
# - Node parser: SentenceSplitter (chunk 500, overlap 50)
# NOTE: ces Settings seront utilis√©s lors de la construction de l'index.
# =========================================================
Settings.embed_model = OllamaEmbedding(
    model_name="nomic-embed-text",
    base_url="http://localhost:11434",
)
Settings.llm = Ollama(
    model="llama3:latest",
    base_url="http://localhost:11434",
    temperature=0.1,
    request_timeout=60,
)
Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)

# =========================================================
# 2) Ingestion des documents
# ---------------------------------------------------------
# - V√©rifie l'existence du r√©pertoire d'entr√©e
# - Charge r√©cursivement tous les fichiers support√©s
# - Transforme les documents en "nodes" (chunks) via le node_parser
# =========================================================
if not os.path.isdir(DATA_DIR):
    raise FileNotFoundError(f"R√©pertoire introuvable: {DATA_DIR}")

print("üì• Chargement des documents‚Ä¶")
documents = SimpleDirectoryReader(DATA_DIR, recursive=True).load_data()
print(f"üìÑ Fichiers d√©tect√©s : {len(documents)}")

nodes = Settings.node_parser.get_nodes_from_documents(documents)
print(f"üß© Chunks g√©n√©r√©s : {len(nodes)}")

# =========================================================
# 3) Initialisation Chroma
# ---------------------------------------------------------
# - PersistentClient : charge/cr√©e le store sur disque (CHROMA_DIR)
# - get_or_create_collection : r√©cup√®re ou cr√©e la collection cible
# - hnsw:space=cosine : m√©trique de similarit√© (cosine) pour HNSW
# - ChromaVectorStore : adapter c√¥t√© LlamaIndex
# - StorageContext    : contexte de stockage pour l'Index
# =========================================================
client = chromadb.PersistentClient(path=CHROMA_DIR)
collection = client.get_or_create_collection(
    name=COLLECTION,
    metadata={"hnsw:space": "cosine"}
)
vector_store = ChromaVectorStore(chroma_collection=collection)
storage_ctx = StorageContext.from_defaults(vector_store=vector_store)

# =========================================================
# 4) Construction de l'index (persistance c√¥t√© Chroma)
# ---------------------------------------------------------
# - VectorStoreIndex construit √† partir des nodes et du storage_ctx
# - show_progress=True : affichage des barres de progression
# - La persistance est g√©r√©e par Chroma (collection + path)
# =========================================================
print("üß† Construction de l'index vectoriel dans Chroma‚Ä¶")
index = VectorStoreIndex(nodes, storage_context=storage_ctx, show_progress=True)

print(f"‚úÖ Index persistant pr√™t dans: {CHROMA_DIR}  (collection: {COLLECTION})")
print(f"üß† Total chunks index√©s : {len(nodes)}")
